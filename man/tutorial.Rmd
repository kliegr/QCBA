---
title: "qCBA tutorial"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## About Quantitative CBA

We propose a postprocessing algorithm for ARC classification algorithm CBA (Liu, 1998), which reverts to the original attribute space to ``edit'' discovered association rules, refining the scope of literals in the antecedent of the rules.  As a consequence, the fit of the individual rules to data improves, rendering some of the rules comprising the model as redundant. These rules can be removed, making the resulting classifier smaller. The viable properties of CBA models that make these rule lists comprehensible, such as one-rule classification and crisp rules, are retained. The postprocessing is conceptually fast, because it is performed on a relatively small number of rules that passed the data coverage pruning in CBA. 

### Novelty

* First "quantitative" association rule classification algorithm.
* Conceptually different from existing quantitative association rule learning algorithms such as QuantMiner or  NAR-Discovery.

### Benchmark

QCBA was evaluated on 22 standard UCI datasets against standard as well as state-of-the-art symbolic classification algorithms (such as RIPPER, FURIA, CBA). The evaluation is fully reproducible using the [arcBench package](https://github.com/kliegr/arcBench) and meets industry standards (same folds, metaparameter optimization, etc.). The evaluation shows that QCBA produces  classifiers with about 1/3 less rules than CBA while retaining comparable accuracy. 

## Library
The QCBA implementation is in Java. It is also available via an [R package wrapper](https://github.com/kliegr/QCBA), which we will used for this example.

```{r results='hide', message=FALSE}
library(qCBA)
attach(humtemp)
```
## Data

Let's look at sample data for this example. There are two explanatory attributes (Temperature and Humidity). The target attribute is preference (e.g. subjective comfort level). 


```{r results='hide', message=FALSE}
attach(humtemp)
```

The first few rows of the data.


```{r}
head(humtemp)
```

And a scatter plot.
```{r}
plot(Humidity,Temperature,pch=as.character(Class))
```

## Building a CBA classifier
Association rule learning requires discretized data.
In this case, we perform simple equidistant binning.


```{r}
  #custom discretization
  data_raw<-humtemp
  data_discr <- humtemp
  temp_breaks <- seq(from=15,to=45,by=5)
  hum_breaks <- c(0,40,60,80,100)
  temp_unique_vals <- setdiff(unique(Temperature),temp_breaks)
  hum_unique_vals <- setdiff(unique(Humidity),hum_breaks)
  data_discr[,1]<-cut(Temperature,breaks=temp_breaks)
  data_discr[,2]<-cut(Humidity,breaks=hum_breaks)
  #change interval syntax from (15,20] to (15;20], which is required by MARC
  data_discr[,1]<-as.factor(unlist(lapply(data_discr[,1], function(x) {gsub(",", ";", x)})))
  data_discr[,2]<-as.factor(unlist(lapply(data_discr[,2], function(x) {gsub(",", ";", x)})))
  
  data_discr[,3] <- as.factor(Class)
  head(data_discr)

```
The discretization split the data space into rectangular regions. Given that we have two attributes, the discovered rule can only correspond to a rectangular region with *borders aligned to the grid*. If we had more than two attributes, the discovered rule would delimit a hypercube.

```{r}
plotGrid <- function(plotFineGrid=TRUE, plotDiscrGrid=TRUE)
{
  if (plotDiscrGrid)
  {

    for (i in temp_breaks[-1])
  {
    abline(h=i, lty=2)
  }
  for (i in hum_breaks[-1])
  {
    abline(v=i, lty=2)
  }
  }
  if (plotFineGrid)
  {
    for (i in temp_unique_vals[-1])
    {
      abline(h=i, lty=3, col="grey")
    }
    for (i in hum_unique_vals[-1])
    {
      abline(v=i, lty=3, col="grey")
    }  
  }
}

plot(Humidity,Temperature,pch=as.character(Class))
plotGrid(FALSE)


```

The next step is mining of association rules. The rule mining is constrained to rules that have values of the Class attribute in the consequent.

```{r results='hide', message=FALSE}
sink("/dev/null")
classAtt="Class"
appearance <- getAppearance(data_discr, classAtt)
txns <- as(data_discr, "transactions")
  rules <- apriori(txns, parameter = list(confidence = 0.5, support= 3/nrow(data_discr), minlen=1, maxlen=5), appearance=appearance)
  inspect(rules)
sink()
inspect(rules)
```

The rules can be visualized in the feature space as rectangular regions. 
<span style="color:red">Class 1 is coded  as red</span>, <span style="color:green">Class 2 as green</span> and <span style="color:blue">Class 4  as blue region</span>.

```{r}
interesting_rule <- 5
#as.character(Class)
plot(Humidity,Temperature,pch=Class,main="Discovered asociation rules",cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)

plotGrid(FALSE)

plotHumTempRule<- function(rules, ruleIndex)
{
  if (typeof(rules)=="S4")
  {
    # rules is a arules rule model
    # sink: inspect also sends rules to the standard output
    sink("/dev/null")
    r <- inspect(rules)[ruleIndex,]
    sink()
    rule <- paste(unlist(r$lhs[1]),collapse='')
    rhs <- paste(unlist(r$rhs[1]),collapse='')
  }
  else
  {
    # rules is a list of rules output by qCBA
    rule <- rules[ruleIndex,1]
    #rule <- rules$rules[ruleIndex]
    rhs <- regmatches(rule,regexec("\\{Class=.*\\}",rule))
  }
  #get color
  if (rhs == "{Class=1}")
  {
    border = "red"
    col=rgb(1.0,0.2,0.2,alpha=0.3)
  }
  else if (rhs == "{Class=2}")
  {
    border = "green"
    col=rgb(0,1,0,alpha=0.3)
  }
  else if (rhs == "{Class=3}")
  {
    border = "black"
    col=rgb(1,1,1,alpha=0.3)
  }
  else if (rhs == "{Class=4}")
  {
    border = "blue"
    col=rgb(0,0,1,alpha=0.3)
  }

  
  temp_coordinates<-unlist(regmatches(rule,regexec("Temperature=.([0-9]+);([0-9]+).",rule)))
  if (length(temp_coordinates)==0)
  {
    #if the temperature literal is missing in the rule, use the following coordinates
    temp_coordinates=c(0,0,50)
  }
  hum_coordinates<-unlist(regmatches(rule,regexec("Humidity=.([0-9]+);([0-9]+).",rule)))
  if (length(hum_coordinates)==0)
  {
    #if the humidity literal is missing in the rule, use the following coordinates
    hum_coordinates=c(0,0,100)
  }
  m <- rect(hum_coordinates[2], temp_coordinates[2], hum_coordinates[3], temp_coordinates[3],border=border,col=col)
}

plotRules <- function(rules)
{
  if (typeof(rules)=="S4") #for arules/cba
  {
    rule_count <- length(rules)
  }
  else #for qcba
  {
    rule_count <- nrow(rules)
  }
  for (i in 1:rule_count)
  {
    plotHumTempRule(rules,i)
  }
}
plotRules(rules)
#plotHumTempRule(rules,1,border = "red",col=rgb(0.8,0.2,0.2,alpha=0.3))
#plotHumTempRule(rules,2,border = "green",col=rgb(0,1,0,alpha=0.3))
#plotHumTempRule(rules,3,border = "blue",col=rgb(0,0,1,alpha=0.3))
#plotHumTempRule(rules,4,border = "blue",col=rgb(0,0,1,alpha=0.3))
#plotHumTempRule(rules,5,border = "blue",col=rgb(0,0,1,alpha=0.3))
#m <- dev.copy(png,filename="cba.png")
#m <- dev.off()
```


Out of the two discovered rules, we will create a CBA classifier.
This means that the rules will be 

1. sorted according to confidence support and length
2. subject to simultaneous data coverage pruning & default rule pruning:
 + data coverage pruning: algorithm iterates through the rules in the sort order removing any rule which does not correctly cover any instances (instances correctly covered by rules above the current rule have been removed)
 + default rule pruning: the algorithm iterates through the rules in the sort order, and cuts off the list once the current rule would result in worse accuracy than a default rule inserted at that place
4. a default rule is inserted at the bottom of the list. Default rule is a rule with empty antecedent and consequent predicting a majority class left in the training data once the previous rules in the classifier were applied.

```{r message=FALSE}
classAtt="Class"
  appearance <- getAppearance(data_discr, classAtt)
  # Note that we are calling `cba_manual()` instead of cba() because we want for demonstration purposes to construct the classifier from a user-generated rule list.
  rmCBA <- cba_manual(data_raw,  rules, txns, appearance$rhs, classAtt, cutp= list(), pruning_options=NULL)
```

In this case, CBA did not remove any rule, bu  added a default rule to the end, which ensures that the rule list covers every possible instance.

```{r}
inspect(rmCBA@rules)
```

CBA added a default rule, therefore the complete instance space is now covered. The green background is associated with the default rule classifying to Class 2.

```{r}
plot(Humidity,Temperature,pch=Class,main="CBA model",cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plotGrid(FALSE)
plotRules(rmCBA@rules)
```




The accuracy of this model on training data:
```{r results='hide', message=FALSE}

  prediction_cba<-predict(rmCBA,data_discr,discretize=FALSE)
  acc_cba <- CBARuleModelAccuracy(prediction_cba, data_discr[[classAtt]])
  print(paste("Accuracy (CBA):",acc_cba))
```
## Postprocessing CBA output with Quantitative CBA (QCBA)
QCBA postprocesses CBA classifier created over discretized numeric attributes. QCBA requires on the input also the original raw undiscretized (continuous) data. 

### Intro to QCBA 
The steps taken by QCBA:

1. **Refit** the data to *finer grid*: this grid has steps corresponding to all unique attribute values appearing in the training data.
2. **Trimming** (optional): the literals in discovered rules are trimmed so that they do not contain regions not covered by data.
3. **Extension**: the ranges of literals in the body of each rule are extended. The range of each literal is increased one literal and one boundary at a time. The extension is generally accepted only if it improves rule confidence. To overcome local minima, the extension process can provisionally accept drop in confidence compared to the seed rule within bounds set by the `minCondImprovement` threshold. There are several extension strategies:
+ `ConfImprovementAgainstLastConfirmedExtension`: extension is accepted only if the extended rule is better by `minImprovement` than the last confirmed extension (or the original rule). This is the most conservative strategy that produces generally produces highest confidence rules.
+ `ConfImprovementAgainstSeedRule`: extension is accepted only if the extended rule is better by minImprovement than the original rule.  This strategy is better fitted to overcome noise in the data.
This strategy can result in "wider" rules while at least still preserving the confidence of the original seed rule (assuming that `minImprovement>=0`).
+ `MinConf`: extension is accepted as long as the extended rule has at least prespecified value of confidence . This strategy produces the widest possible extension, producing rules that at least preserve the confidence threshold `minConf`, which can be the same as confidence threshold used for CBA classifier building.
4. **Continuous pruning** (optional): this essentially corresponds to second iteration of data coverage pruning (first was performed during CBA classifier building), which is performed immediately after a rule is extended.
5. **Postpruning (optional)**: second iteration of data coverage pruning is performed *after* all rules have been extended and resorted.  



###QCBA model

To build a model, `qcba` needs a cba model and raw (undiscretized) data. Note that number of additional parameters can be also specified - these were left to their default values.
```{r results='hide'}
trim_literal_boundaries = TRUE
rmqCBA <- qcba(cbaRuleModel=rmCBA,datadf=data_raw,extensionStrategy= "ConfImprovementAgainstLastConfirmedExtension", trim_literal_boundaries = trim_literal_boundaries, continuousPruning = FALSE, postpruning = TRUE, minImprovement=0, minCondImprovement=-1, minConf = 0.5, createHistorySlot=TRUE,loglevel = "WARNING")
print(rmqCBA@rules)

```

As we can notice, the number of rules on the output decreased. After the rule boundaries have been extended, postpruning removed one rule. The default rule was recomputed, and now classifies to blue (Class 4).
```{r}
plot(Humidity,Temperature,pch=Class,main="QCBA model",cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plotGrid(FALSE)
plotRules(rmqCBA@rules)
```

### Under the QCBA hood

#### The finer grid
CBA rules stick to a grid that corresponds to results of discretization (figure left below). The grid used by QCBA corresponds to all unique values appearing in the training data (figure right below).

```{r}
plot(Humidity,Temperature,pch=as.character(Class))
plotGrid(FALSE)
plot(Humidity,Temperature,pch=as.character(Class))
plotGrid(TRUE)

```


Rule  #2 from the QCBA classifier plotted on the finer grid. 

```{r}
plot(Humidity,Temperature,pch=Class, main="QCBA model",cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plotGrid()
plotHumTempRule(rmqCBA@rules,2)
m<-dev.copy(png,filename="qcba.png")
m<-dev.off()
```

### Extension, literal trinming and conditional accept 

Here, we will illustrate the extension process on a sequence of plots.

```{r fig.width=7, fig.height=6, fig.show='animate'}
plotHumTempRuleDelta<- function(rules, ruleIndex2,ruleIndex1, border = "red",col=rgb(0,0,1,alpha=0.3) )
{
  rule1 <- rules$rules[ruleIndex1]
  rule2 <- rules$rules[ruleIndex2]
  
  temp_coordinates1 <- unlist(regmatches(rule1,regexec("Temperature=.([0-9]+);([0-9]+).",rule1)))
  temp_coordinates2 <- unlist(regmatches(rule2,regexec("Temperature=.([0-9]+);([0-9]+).",rule2)))
  hum_coordinates1 <- unlist(regmatches(rule1,regexec("Humidity=.([0-9]+);([0-9]+).",rule1)))
  hum_coordinates2 <- unlist(regmatches(rule2,regexec("Humidity=.([0-9]+);([0-9]+).",rule2)))
  
  r1_bottom <- temp_coordinates1[2]
  r1_top <- temp_coordinates1[3] 
  r1_left <- hum_coordinates1[2]
  r1_right <- hum_coordinates1[3] 
  r2_bottom <- temp_coordinates2[2]
  r2_top <- temp_coordinates2[3] 
  r2_left <- hum_coordinates2[2]
  r2_right <- hum_coordinates2[3]  
  
  if (r2_right>r1_right)
  {
    r_right<- r2_right
    r_left <- r1_right
    r_top <- r1_top
    r_bottom <- r1_bottom
  }
  else if(r2_left>r1_left)
  {
    r_right<- r2_left
    r_left <- r1_left
    r_top <- r1_top
    r_bottom <- r1_bottom
  }
  else if  (r2_top>r1_top)
  {
    r_right<- r1_right
    r_left <- r1_left
    r_top <- r2_top
    r_bottom <- r1_top
  }  
  else if  (r2_bottom<r1_bottom)
  {
    r_right<- r1_right
    r_left <- r1_left
    r_top <- r1_bottom
    r_bottom <- r2_bottom
  }    
  m <- rect(r_left,r_bottom,r_right,r_top,border=border,col=col)
}

inspected_rule_RID <- "2"
extendHistory <- rmqCBA@history[rmqCBA@history$RID==inspected_rule_RID,]
base_rule_in_history <- 1
if (trim_literal_boundaries == TRUE)
{
# the base confidence will be taken from the trimmed rule, which is the second rule in history
 base_rule_in_history <- 2
}
seedRuleConf <- rmqCBA@history[rmqCBA@history$RID==inspected_rule_RID,][base_rule_in_history,5]
for (i in 1:nrow(extendHistory)) {
  titles=c("Rule is refit to the finer grid","Rule is trimmed", rep("Rule is extended",nrow(extendHistory)-3), "Final rule: no other extend was succcessful")
  curRuleConf <- extendHistory[i,5]
    if (seedRuleConf > curRuleConf  && i>1){
      titles[i] <- paste(titles[i]," (Conditional accept)")
    }
  
  plot(Humidity,Temperature,pch=Class, main=titles[i],cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1, sub=paste(extendHistory[i,3], "Supp:",round(extendHistory[i,4],2)," Conf:",round(curRuleConf,2)))
  plotGrid(TRUE,FALSE)
  plotHumTempRule(extendHistory[3],i)
  if (seedRuleConf > curRuleConf && i>1)
  {
    plotHumTempRuleDelta(extendHistory,i-1,i,border="red",col=rgb(1,0,0,alpha=1.0))
  }  
}
```

The process proceeds as follows:
1. Rule is refit to finer grid. Boundaries shrink, confidence and support is unaffected.
2. Rule is trimmed: it is shaved of one misclassified data point, confidence rises from 0.6 to 0.75
3.-9. Rule coverage is extended. Confidence and support is unaffected.
10. Left boundary on Humidity conditionally extends - one additional misclassified and one correctly classified data point is covered. Confidence drops to 0.67.
11. Left boundary  on Humidity conditionally extends - one additional correctly classified point is covered. Confidence increases to 0.71, but is still below 0.75.
12. Left boundary  on Humidity extends - one additional correctly classified point is covered. Confidence returns 0.75.
13. Left boundary  on Humidity extends. Confidence and support is unaffected. Final rule.

Note that QCBA improved confidence of rule 2 from initial value of 0.6 to 0.75  and support from 0.08 to 0.17.


## Parameters and optimizations

### Speeding up conditional accept
The default value of the minCondImprovement parameter is -1. This parameter value ensures exhaustive search for extensions, but can be slow on large datasets or datasets with many distinct values. The closer this parameter value will be to 0, the faster the execution will generally be. 

